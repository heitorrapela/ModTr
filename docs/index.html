<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight: 300;
		font-size: 18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-weight: 300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	a:link,
	a:visited {
		color: #1367a7;
		text-decoration: none;
	}

	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35),
			/* The top layer shadow */
			5px 5px 0 0px #fff,
			/* The second layer */
			5px 5px 1px 1px rgba(0, 0, 0, 0.35),
			/* The second layer shadow */
			10px 10px 0 0px #fff,
			/* The third layer */
			10px 10px 1px 1px rgba(0, 0, 0, 0.35),
			/* The third layer shadow */
			15px 15px 0 0px #fff,
			/* The fourth layer */
			15px 15px 1px 1px rgba(0, 0, 0, 0.35),
			/* The fourth layer shadow */
			20px 20px 0 0px #fff,
			/* The fifth layer */
			20px 20px 1px 1px rgba(0, 0, 0, 0.35),
			/* The fifth layer shadow */
			25px 25px 0 0px #fff,
			/* The fifth layer */
			25px 25px 1px 1px rgba(0, 0, 0, 0.35);
		/* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35);
		/* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35),
			/* The top layer shadow */
			5px 5px 0 0px #fff,
			/* The second layer */
			5px 5px 1px 1px rgba(0, 0, 0, 0.35),
			/* The second layer shadow */
			10px 10px 0 0px #fff,
			/* The third layer */
			10px 10px 1px 1px rgba(0, 0, 0, 0.35);
		/* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}

	hr {
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>

<head>
	<title>Modality Translation for Object Detection Adaptation Without Forgetting Prior Knowledge</title>
	<meta property="og:image" content="" />
	<meta property="og:title" content="Modality Translation for Object Detection Adaptation Without Forgetting Prior Knowledge" />
</head>

<body>
	<br>
	<center>
		<span style="font-size:42px"><a style=color:red>Mod</a>ality
			<a style=color:red>Tr</a>anslation for Object Detection Adaptation Without Forgetting Prior Knowledge</span>
		<table align=center width=600px>
			<tr>

				<td align=center width=120px>
					<center>
						<span style="font-size:24px"><a href="https://heitorrapela.github.io/"  target="_blank">Heitor Medeiros</a></span>
					</center>
				</td>

				<td align=center width=120px>
					<center>
						<span style="font-size:24px"><a href="https://github.com/Masseeh"  target="_blank">Masih Aminbeidokhti</a></span>
					</center>
				</td>		

				<td align=center width=120px>
					<center>
						<span style="font-size:24px"><a href="https://www.linkedin.com/in/fidel-guerrero-pena/"  target="_blank">Fidel A.
								G. Pena</a></span>
					</center>
				</td>

		</table>

		<table align=center width=600px>
			<tr>
				<td align=center width=120px>
					<center>
						<span style="font-size:24px"><a href="https://www.linkedin.com/in/david-latortue-a3b37bb6/"  target="_blank">David Latortue</a></span>
					</center>
				</td>
				<td align=center width=120px>
					<center>
						<span style="font-size:24px"><a
								href="https://scholar.google.com/citations?hl=en&user=TmfbdagAAAAJ&view_op=list_works&sortby=pubdate"  target="_blank">Eric Granger</a></span>
					</center>
				</td>

				<td align=center width=120px>
					<center>
						<span style="font-size:24px"><a
								href="https://scholar.google.com/citations?hl=en&user=aVfyPAoAAAAJ&view_op=list_works&sortby=pubdate"  target="_blank">Marco
								Pedersoli</a></span>
					</center>
				</td>
		</table>
		<span style="font-size:30px">ECCV 2024</span>

		<table align=center width=650px>
			<tr>
				<!-- <td align=center width=150px>
	  					<center>
	  						<span style="font-size:24px"><a href=''> [Demo]</a>
		  		  		</center>
		  		  	  </td> -->
				<td align=center width=150px>
					<center>
						<span style="font-size:24px"><a href='https://github.com/heitorrapela/ModTr' target="_blank">
								[GitHub]</a></span>
					</center>
				</td>
				<!-- <td align=center width=150px>
	  					<center>
	  						<span style="font-size:24px"><a href='https://www.youtube.com/watch?v=4oJfQwBnfm8&t=1s' target="_blank"> [Talk]</a></span>
		  		  		</center>
		  		  	  </td> -->
				<!-- <td align=center width=150px>
	  					<center>
	  						<span style="font-size:24px"><a href=''> [Slides]</a></span>
		  		  		</center>
		  		  	  </td> -->
				<td align=center width=150px>
					<center>
						<span style="font-size:24px"><a
								href='https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/12401.pdf'  target="_blank">
								[Paper]</a></span>
					</center>
				</td>

				<td align=center width=150px>
	  					<center>
	  						<span style="font-size:24px"><a href='./images/modtr_eccv2024_poster.pdf'  target="_blank"> [Poster]</a></span>
		  		  		</center>
		  		  	  </td>

			</tr>
			<tr>
		</table>
	</center>

	<!--   		  <br><br>
		  <hr> -->

	<br>
	
	<table align=center width=100px>
		<tr>
			<td align=center width=75px>
				<center>
			<td><a href='https://github.com/heitorrapela/ModTr' target="_blank"><img class="round" style="height:350px"
						src="./images/modtr_main.png" /></a></td>
			</center>
		</tr>
	</table>

	</div>
	<br><br>
	<hr>


	<table align=center width=850px>
		<center>
			<h1>Abstract</h1>
		</center>
	</table>
	A common practice in deep learning involves training large neural networks on massive datasets to achieve high accuracy across various domains and tasks. While this approach works well in many application areas, it often fails drastically when processing data from a new modality with a significant distribution shift from the data used to pre-train the model. This paper focuses on adapting a large object detection model trained on RGB images to new data extracted from IR images with a substantial modality shift. We propose Modality Translator (ModTr) as an alternative to the common approach of fine-tuning a large model to the new modality. ModTr adapts the IR input image with a small transformation network trained to directly minimize the detection loss. The original RGB model can then work on the translated inputs without any further changes or fine-tuning to its parameters. Experimental results on translating from IR to RGB images on two well-known datasets show that our simple approach provides detectors that perform comparably or better than standard fine-tuning, without forgetting the knowledge of the original model. This opens the door to a more flexible and efficient service-based detection pipeline, where a unique and unaltered server, such as an RGB detector, runs constantly while being queried by different modalities, such as IR with the corresponding translations model.
	<br><br>
	<hr>


	<center>
		<h1>Try our code</h1>
	</center>

	<table align=center width=850px>
		<tr>
			<td width=400px>
				<center>
					<a href="https://github.com/heitorrapela/ModTr" target="_blank"><img class="rounded" src="./images/modtr_imgs.png" height="300px"></img>
						</href></a><br>
				</center>
			</td>
		</tr>
		<td width=400px>
			<br>
			<center>
				<span style="font-size:14px"><i>Bounding box predictions over different adaptations of the RGB detector (Faster R-CNN) for IR images on two benchmarks: LLVIP and FLIR. Yellow and red boxes show the ground truth and predicted detections, respectively. In a) we see the RGB data. In b) FastCUT is an unsupervised image translation approach that takes as input infrared images (IR) and produces pseudo-RGB images. It does not focus on detection and requires both modalities for training. In c) we have fine-tuning, which is the standard approach to adapting the detector to the new modality. It requires only IR data but forgets the original knowledge of the original RGB detector. Finally, in d) is the ModTr, which focuses the translation on detection, requires only IR data and does not forget the original knowledge so that it can be reused for other tasks. Bounding box predictions for other detectors are provided in the supplementary material.</i>
			</center>
		</td>
	</table>




	<br>
	<hr>


	<table align=center width=425px>
		<center>
			<h1>Paper and Supplementary Material</h1>
		</center>
		<tr>

			<td><a
					href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/12401.pdf" target="_blank"><img
						class="layered-paper-big" style="height:175px" src="./images/paper_pdf_thumb.png" /></a></td>
			<td><span style="font-size:14pt">
					Heitor Rapela Medeiros, Masih Aminbeidokhti, Fidel A. Guerrero Pena, David Latortue, Eric Granger, Marco Pedersoli
					<br><br>
					Modality Translation for Object Detection Adaptation Without Forgetting Prior Knowledge.<br>
					In ECCV, 2024.<br><br>
					(hosted on <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/12401.pdf"  target="_blank">ECCV2024</a>)</a>
			</td>
			</td>
		</tr>
	</table>
	<br>


	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt">
					<center>
						<a href="./bibtex.txt"  target="_blank">[Bibtex]</a>
					</center>
			</td>
		</tr>
	</table>

	<hr>

	<a name="bw_legacy"></a>
	<center>
		<h1>Experiments and Results</h1>
	</center>

	<left>
		<h3>- Comparison with Translation Approaches.</h3>
	</left>
	<table align=center width=850px>
		<tr>
			<td width=400px>
				<center>
					<a href="./images/exps01_i2i.png" target="_blank"><img class="rounded" src="./images/exps01_i2i.png" height="500px"></img></href>
						</a><br>
				</center>
			</td>
		</tr>
	</table>

	<br>

	<span style="font-size:14pt"> <b> Table 1</b></span>: Detection performance (AP) of ModTr versus baseline image-to-image methods to
	translate the IR to RGB-like images, using three different detectors (FCOS, RetinaNet, and Faster
	R-CNN). The methods were evaluated on IR test set of LLVIP and FLIR datasets. The RGB column indicates if the method required access to RGB images during training, and Box refers to
	the use of ground truth boxes during training.



	

	<left>
		<h3>- Translation vs. Fine-tuning.</h3>
	</left>
	<table align=center width=850px>
		<tr>
			<td width=400px>
				<center>
					<a href="./images/exps02_ft.png" target="_blank"><img class="rounded" src="./images/exps02_ft.png" height="500px"></img></href>
						</a><br>
				</center>
			</td>
		</tr>
	</table>

	<br>

	<span style="font-size:14pt"> <b> Table 2</b></span>: Detection performance (AP) of ModTr versus baseline fine-tuning (FT) of the detector,
	FT of the head and LoRA [18], using three different detectors (FCOS, RetinaNet, and Faster RCNN. The methods were evaluated on IR test set of LLVIP and FLIR datasets. Results with "-"
	diverged from the optimization.


	<left>
		<h3>- Different Backbones for ModTr.</h3>
	</left>
	<table align=center width=850px>
		<tr>
			<td width=400px>
				<center>
					<a href="./images/exps03_diffback.png" target="_blank"><img class="rounded" src="./images/exps03_diffback.png" height="500px"></img></href>
						</a><br>
				</center>
			</td>
		</tr>
	</table>

	<br>

	<span style="font-size:14pt"> <b> Table 3</b></span>: Detection performance (AP) of ModTr with different backbones for the translation networks with different numbers of parameters, using three different detectors (FCOS, RetinaNet,
	and Faster R-CNN). The methods were evaluated on IR test set of LLVIP and FLIR datasets.



	<left>
		<h3>- Knowledge Preservation through Input Modality Translation.</h3>
	</left>
	<table align=center width=850px>
		<tr>
			<td width=400px>
				<center>
					<a href="./images/exps04_knowledgepres.png" target="_blank"><img class="rounded" src="./images/exps04_knowledgepres.png" height="500px"></img></href>
						</a><br>
				</center>
			</td>
		</tr>
	</table>

	<br>

	<span style="font-size:14pt"> <b> Table 4</b></span>: Detection performance (AP) of knowledge preserving techniques N-Detectors, 1-
	Detector, and N-ModTr-1-Detector, using three different detectors (FCOS, RetinaNet, and Faster
	R-CNN). The methods were evaluated on COCO and IR test sets of LLVIP and FLIR datasets.


	<left>
		<h3>- Visualization of ModTr Translated Images.</h3>
	</left>
	<table align=center width=850px>
		<tr>
			<td width=400px>
				<center>
					<a href="./images/exps05_imgs.png" target="_blank"><img class="rounded" src="./images/exps05_imgs.png" height="500px"></img></href>
						</a><br>
				</center>
			</td>
		</tr>
	</table>

	<br>

	<span style="font-size:14pt"> <b> Figure 3</b></span>: Illustration of a sequence of 8 images of LLVIP and FLIR dataset for Faster R-CNN.
	For each dataset, the first row is the RGB modality, followed by the IR modality and different representations created by ModTr.




	<left>
		<h3>- Fine-tuning of ModTr and the Detector.</h3>
	</left>
	<table align=center width=850px>
		<tr>
			<td width=400px>
				<center>
					<a href="./images/exps06_ftmodtrdet.png" target="_blank"><img class="rounded" src="./images/exps06_ftmodtrdet.png" height="300px"></img></href>
						</a><br>
				</center>
			</td>
		</tr>
	</table>

	<br>

	<span style="font-size:14pt"> <b> Figure 4</b></span>:  Comparison of the performance of fine-tuning the ModTr and normal fine-tuning on the
	FLIR dataset for the three different detectors (FCOS, RetinaNet, and Faster R-CNN). In blue, the Fine-tuning; in orange, the ModTr⊙, and in green, ModTr⊙ + FT.






	<br><br><br><br>

	<br>
	<hr>
	<br>

	<table align=center width=1100px>
		<tr>
			<td width=400px>
				<left>
					<center>
						<h1>Acknowledgements</h1>
					</center>

					This work was supported in part by Distech Controls Inc., the Natural Sciences and
					Engineering Research Council of Canada, the Digital Research Alliance of Canada,
					and MITACS.
				</left>
			</td>
		</tr>
	</table>


	<br>
	<hr>
	<br>

	<table align=center width=850px>
		<br>
		<tr>
			<td width=400px>
				<center>
					<a href="https://www.etsmtl.ca/" target="_blank"><img class="rounded" src="./images/ets.png" height="100px"></img>
						</href></a><br>
				</center>
			</td>
			<td width=400px>
				<center>
					<a href="https://etsmtl.ca/" target="_blank"><img class="rounded" src="./images/ills.png" height="100px"></img>
						</href></a><br>
				</center>
			</td>

			<td width=400px>
				<center>
					<a href="https://etsmtl.ca/" target="_blank"><img class="rounded" src="./images/livia.png" height="100px"></img>
						</href></a><br>
				</center>
			</td>
		</tr>



	</table>

	<br><br>

	<script>
		(function (i, s, o, g, r, a, m) {
			i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
				(i[r].q = i[r].q || []).push(arguments)
			}, i[r].l = 1 * new Date(); a = s.createElement(o),
				m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
		})(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

		ga('create', 'UA-75863369-1', 'auto');
		ga('send', 'pageview');

	</script>

</body>

</html>